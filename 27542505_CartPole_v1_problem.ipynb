{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# To build an agent using reinforcement learning, I chose a problem from the AI Gymnasium - the CartPole-v1 - that the agent will solve.\n",
        "\n",
        "## The problem involves balancing a pole on top of a moving cart by controlling the cart's movement. The goal is to keep the pole upright for as long as possible.\n",
        "\n",
        "### To build an agent that can solve this problem using reinforcement learning, the following steps were followed:"
      ],
      "metadata": {
        "id": "Jr5aCjawhFyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Step to initialize the environment."
      ],
      "metadata": {
        "id": "-0J9g-D_lP8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_step_api=True"
      ],
      "metadata": {
        "id": "8hyNJZsSlXCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Importing the necessary librarie:"
      ],
      "metadata": {
        "id": "p4yk05sLlVY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "jDdnNckgldjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Creating an environment for the problem:"
      ],
      "metadata": {
        "id": "Xpf3-T9Pla7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make('CartPole-v1')\n"
      ],
      "metadata": {
        "id": "vbbeyVA1mFIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Define the neural network model:"
      ],
      "metadata": {
        "id": "5jViBjrJmHq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model"
      ],
      "metadata": {
        "id": "nVb5k8YLmPK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Defining the Agent class:"
      ],
      "metadata": {
        "id": "2wBDQ1HkmMbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.model = build_model(state_size, action_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Use the model to predict the Q values for each action\n",
        "        q_values = self.model.predict(state)\n",
        "        # Choose the action with the highest Q value\n",
        "        action = np.argmax(q_values[0])\n",
        "        return action\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        # Use the model to predict the Q values for the current state\n",
        "        q_values = self.model.predict(state)\n",
        "        # Use the model to predict the Q values for the next state\n",
        "        next_q_values = self.model.predict(next_state)\n",
        "        # Update the Q value for the chosen action using the Bellman equation\n",
        "        q_values[0][action] = reward + 0.99 * np.max(next_q_values) * (1 - done)\n",
        "        # Train the model on the updated Q values\n",
        "        self.model.fit(state, q_values, verbose=0)\n"
      ],
      "metadata": {
        "id": "OJ4u0qd3md22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Test and training the agent using the Q-learning algorithm:"
      ],
      "metadata": {
        "id": "EQPBjQ0jmktW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O514pGMUYy7T"
      },
      "outputs": [],
      "source": [
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = Agent(state_size, action_size)\n",
        "\n",
        "for episode in range(15):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    done = False\n",
        "    time_step = 0\n",
        "    while not done:\n",
        "        # Choose an action\n",
        "        action = agent.act(state)\n",
        "        # Take the action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        # Train the agent on the experience\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        # Update the state\n",
        "        state = next_state\n",
        "        time_step += 1\n",
        "    print(\"Episode:\", episode, \"Time step:\", time_step)\n"
      ]
    }
  ]
}